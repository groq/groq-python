# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

from typing import Dict, List, Optional
from typing_extensions import Literal

from ..._models import BaseModel
from ..completion_usage import CompletionUsage
from .chat_completion_message import ChatCompletionMessage
from .chat_completion_token_logprob import ChatCompletionTokenLogprob

__all__ = [
    "ChatCompletion",
    "Choice",
    "ChoiceLogprobs",
    "McpListTool",
    "McpListToolTool",
    "UsageBreakdown",
    "UsageBreakdownModel",
    "XGroq",
    "XGroqDebug",
    "XGroqUsage",
]


class ChoiceLogprobs(BaseModel):
    """Log probability information for the choice."""

    content: Optional[List[ChatCompletionTokenLogprob]] = None
    """A list of message content tokens with log probability information."""


class Choice(BaseModel):
    finish_reason: Literal["stop", "length", "tool_calls", "function_call"]
    """The reason the model stopped generating tokens.

    This will be `stop` if the model hit a natural stop point or a provided stop
    sequence, `length` if the maximum number of tokens specified in the request was
    reached, `tool_calls` if the model called a tool, or `function_call`
    (deprecated) if the model called a function.
    """

    index: int
    """The index of the choice in the list of choices."""

    logprobs: Optional[ChoiceLogprobs] = None
    """Log probability information for the choice."""

    message: ChatCompletionMessage
    """A chat completion message generated by the model."""


class McpListToolTool(BaseModel):
    annotations: Optional[object] = None
    """Additional metadata for the tool."""

    description: Optional[str] = None
    """Description of what the tool does."""

    input_schema: Optional[Dict[str, object]] = None
    """JSON Schema describing the tool's input parameters."""

    name: Optional[str] = None
    """The name of the tool."""


class McpListTool(BaseModel):
    id: Optional[str] = None
    """Unique identifier for this tool list response."""

    server_label: Optional[str] = None
    """Human-readable label for the MCP server."""

    tools: Optional[List[McpListToolTool]] = None
    """Array of discovered tools from the server."""

    type: Optional[str] = None
    """The type identifier."""


class UsageBreakdownModel(BaseModel):
    model: str
    """The name/identifier of the model used"""

    usage: CompletionUsage
    """Usage statistics for the completion request."""


class UsageBreakdown(BaseModel):
    """
    Detailed usage breakdown by model when multiple models are used in the request for compound AI systems.
    """

    models: List[UsageBreakdownModel]
    """List of models used in the request and their individual usage statistics"""


class XGroqDebug(BaseModel):
    """Debug information including input and output token IDs and strings.

    Only present when debug=true in the request.
    """

    input_token_ids: Optional[List[int]] = None
    """Token IDs for the input."""

    input_tokens: Optional[List[str]] = None
    """Token strings for the input."""

    output_token_ids: Optional[List[int]] = None
    """Token IDs for the output."""

    output_tokens: Optional[List[str]] = None
    """Token strings for the output."""


class XGroqUsage(BaseModel):
    """Additional Groq-specific usage metrics (hardware cache statistics)."""

    dram_cached_tokens: Optional[int] = None
    """Number of tokens served from DRAM cache."""

    sram_cached_tokens: Optional[int] = None
    """Number of tokens served from SRAM cache."""


class XGroq(BaseModel):
    """Groq-specific metadata for non-streaming chat completion responses."""

    id: str
    """
    A groq request ID which can be used to refer to a specific request to groq
    support.
    """

    debug: Optional[XGroqDebug] = None
    """Debug information including input and output token IDs and strings.

    Only present when debug=true in the request.
    """

    seed: Optional[int] = None
    """The seed used for the request.

    See the seed property on CreateChatCompletionRequest for more details.
    """

    usage: Optional[XGroqUsage] = None
    """Additional Groq-specific usage metrics (hardware cache statistics)."""


class ChatCompletion(BaseModel):
    """
    Represents a chat completion response returned by model, based on the provided input.
    """

    id: str
    """A unique identifier for the chat completion."""

    choices: List[Choice]
    """A list of chat completion choices.

    Can be more than one if `n` is greater than 1.
    """

    created: int
    """The Unix timestamp (in seconds) of when the chat completion was created."""

    model: str
    """The model used for the chat completion."""

    object: Literal["chat.completion"]
    """The object type, which is always `chat.completion`."""

    mcp_list_tools: Optional[List[McpListTool]] = None
    """List of discovered MCP tools from connected servers."""

    service_tier: Optional[Literal["auto", "on_demand", "flex", "performance"]] = None
    """The service tier used for the request."""

    system_fingerprint: Optional[str] = None
    """This fingerprint represents the backend configuration that the model runs with.

    Can be used in conjunction with the `seed` request parameter to understand when
    backend changes have been made that might impact determinism.
    """

    usage: Optional[CompletionUsage] = None
    """Usage statistics for the completion request."""

    usage_breakdown: Optional[UsageBreakdown] = None
    """
    Detailed usage breakdown by model when multiple models are used in the request
    for compound AI systems.
    """

    x_groq: Optional[XGroq] = None
    """Groq-specific metadata for non-streaming chat completion responses."""
