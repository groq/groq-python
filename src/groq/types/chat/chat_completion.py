# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

from typing import Dict, List, Optional
from typing_extensions import Literal

from ..._models import BaseModel
from ..completion_usage import CompletionUsage
from .chat_completion_message import ChatCompletionMessage
from .chat_completion_token_logprob import ChatCompletionTokenLogprob

__all__ = [
    "ChatCompletion",
    "Choice",
    "ChoiceLogprobs",
    "McpListTool",
    "McpListToolTool",
    "UsageBreakdown",
    "UsageBreakdownModel",
]


class ChoiceLogprobs(BaseModel):
    content: Optional[List[ChatCompletionTokenLogprob]] = None
    """A list of message content tokens with log probability information."""


class Choice(BaseModel):
    finish_reason: Literal["stop", "length", "tool_calls", "function_call"]
    """The reason the model stopped generating tokens.

    This will be `stop` if the model hit a natural stop point or a provided stop
    sequence, `length` if the maximum number of tokens specified in the request was
    reached, `tool_calls` if the model called a tool, or `function_call`
    (deprecated) if the model called a function.
    """

    index: int
    """The index of the choice in the list of choices."""

    logprobs: Optional[ChoiceLogprobs] = None
    """Log probability information for the choice."""

    message: ChatCompletionMessage
    """A chat completion message generated by the model."""


class McpListToolTool(BaseModel):
    description: str
    """The description of the discovered tool"""

    input_schema: Dict[str, object]
    """JSON schema describing the tool's input parameters"""

    name: str
    """The name of the discovered tool"""


class McpListTool(BaseModel):
    id: str
    """Unique identifier for the MCP discovery operation"""

    server_label: str
    """Label or identifier for the MCP server"""

    tools: List[McpListToolTool]
    """List of tools discovered from the MCP server"""

    type: Literal["mcp_list_tools"]
    """The type of output, always "mcp_list_tools" """


class UsageBreakdownModel(BaseModel):
    model: str
    """The name/identifier of the model used"""

    usage: CompletionUsage
    """Usage statistics for the completion request."""


class UsageBreakdown(BaseModel):
    models: List[UsageBreakdownModel]
    """List of models used in the request and their individual usage statistics"""


class ChatCompletion(BaseModel):
    id: str
    """A unique identifier for the chat completion."""

    choices: List[Choice]
    """A list of chat completion choices.

    Can be more than one if `n` is greater than 1.
    """

    created: int
    """The Unix timestamp (in seconds) of when the chat completion was created."""

    model: str
    """The model used for the chat completion."""

    object: Literal["chat.completion"]
    """The object type, which is always `chat.completion`."""

    mcp_list_tools: Optional[List[McpListTool]] = None
    """List of tools discovered from MCP servers during the request."""

    system_fingerprint: Optional[str] = None
    """This fingerprint represents the backend configuration that the model runs with.

    Can be used in conjunction with the `seed` request parameter to understand when
    backend changes have been made that might impact determinism.
    """

    usage: Optional[CompletionUsage] = None
    """Usage statistics for the completion request."""

    usage_breakdown: Optional[UsageBreakdown] = None
    """
    Detailed usage breakdown by model when multiple models are used in the request
    for compound AI systems.
    """
